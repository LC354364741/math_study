### 首先我们看一下 Fisher Information 的定义：

* 假设你观察到 i.i.d 的数据 ![X_1, X_2, \ldots X_n](https://www.zhihu.com/equation?tex=X_1%2C+X_2%2C+%5Cldots+X_n) 服从一个概率分布![f(X; \theta)](https://www.zhihu.com/equation?tex=f%28X%3B+%5Ctheta%29),![\theta](https://www.zhihu.com/equation?tex=%5Ctheta)是你的目标参数（for simplicity， 这里![\theta](https://www.zhihu.com/equation?tex=%5Ctheta)是个标量，且不考虑 nuissance parameter），那么你的似然函数（likelihood）就是：![L(\bold{X};\theta) = \prod_{i=1}^n f(X_i;\theta)](https://www.zhihu.com/equation?tex=L%28%5Cbold%7BX%7D%3B%5Ctheta%29+%3D+%5Cprod_%7Bi%3D1%7D%5En+f%28X_i%3B%5Ctheta%29),为了解得Maximum Likelihood Estimate(MLE)，我们要让log likelihood的一阶导数得0，然后解这个方程，得到![\hat{\theta}_{MLE}](https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_%7BMLE%7D)这个log likelihood的一阶导数也叫，Score function ：![S(\bold{X};\theta) = \sum_{i=1}^n \frac{\partial log f(X_i;\theta)}{\partial \theta}](https://www.zhihu.com/equation?tex=S%28%5Cbold%7BX%7D%3B%5Ctheta%29+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac%7B%5Cpartial+log+f%28X_i%3B%5Ctheta%29%7D%7B%5Cpartial+%5Ctheta%7D)

那么Fisher Information，用![I(\theta)](https://www.zhihu.com/equation?tex=I%28%5Ctheta%29)表示的定义就是这个Score function的二阶矩（second moment）![I(\theta) = E[S(X;\theta)^2] ](https://www.zhihu.com/equation?tex=I%28%5Ctheta%29+%3D+E%5BS%28X%3B%5Ctheta%29%5E2%5D+)。一般情况下可以很容易地证明，![E[S(\bold{X};\theta)]= 0](https://www.zhihu.com/equation?tex=E%5BS%28%5Cbold%7BX%7D%3B%5Ctheta%29%5D%3D+0), 从而得到：
![I(\theta) = E[S(X;\theta)^2]-E[S(X;\theta)]^2 = Var[S(X;\theta)]](https://www.zhihu.com/equation?tex=I%28%5Ctheta%29+%3D+E%5BS%28X%3B%5Ctheta%29%5E2%5D-E%5BS%28X%3B%5Ctheta%29%5D%5E2+%3D+Var%5BS%28X%3B%5Ctheta%29%5D)
于是得到了**Fisher Information的第一条数学意义：就是用来估计MLE的方程的方差**。它的直观表述就是，随着收集的数据越来越多，这个方差由于是一个Independent sum的形式，也就变的越来越大，也就象征着得到的信息越来越多。

而且，如果log likelihood二阶可导，在一般情况下（under specific regularity conditions）可以很容易地证明:
![E[S(\bold{X};\theta)^2] = -E(\frac{\partial^2}{\partial \theta^2}log L(\bold{X};\theta))](https://www.zhihu.com/equation?tex=E%5BS%28%5Cbold%7BX%7D%3B%5Ctheta%29%5E2%5D+%3D+-E%28%5Cfrac%7B%5Cpartial%5E2%7D%7B%5Cpartial+%5Ctheta%5E2%7Dlog+L%28%5Cbold%7BX%7D%3B%5Ctheta%29%29)
于是得到了**Fisher Information的第二条数学意义：log likelihood在参数真实值处的负二阶导数的期望**。这个意义好像很抽象，但其实超级好懂。
首先看一下一个normalized Bernoulli log likelihood长啥样：

![img](https://pic3.zhimg.com/80/28c4c679b6758707ed779c066d0e8e3a_hd.jpg)

对于这样的一个log likelihood function，它越平而宽，就代表我们对于参数估计的能力越差，它高而窄，就代表我们对于参数估计的能力越好，也就是信息量越大。而这个log likelihood在参数真实值处的负二阶导数，就反应了这个log likelihood在顶点处的弯曲程度，弯曲程度越大，整个log likelihood的形状就越偏向于高而窄，也就代表掌握的信息越多。



然后，在一般情况下，通过对score function在真实值处泰勒展开，然后应用中心极限定理，弱大数定律，依概率一致收敛，以及Slutsky定理，可以证明MLE的渐进分布的方差是![I^{-1}(\theta)](https://www.zhihu.com/equation?tex=I%5E%7B-1%7D%28%5Ctheta%29)

，即![Var(\hat{\theta}_{MLE}) = I^{-1}(\theta)](https://www.zhihu.com/equation?tex=Var%28%5Chat%7B%5Ctheta%7D_%7BMLE%7D%29+%3D+I%5E%7B-1%7D%28%5Ctheta%29), 这也就是Fisher Information的第三条数学意义。不过这样说不严谨，严格的说，应该是 ![\sqrt{n}(\hat{\theta}_{MLE}-\theta) \xrightarrow{D} N(0,I^*(\theta)^{-1})](https://www.zhihu.com/equation?tex=%5Csqrt%7Bn%7D%28%5Chat%7B%5Ctheta%7D_%7BMLE%7D-%5Ctheta%29+%5Cxrightarrow%7BD%7D+N%280%2CI%5E%2A%28%5Ctheta%29%5E%7B-1%7D%29), 这里![I^*(\theta)](https://www.zhihu.com/equation?tex=I%5E%2A%28%5Ctheta%29)是当只观察到一个X值时的Fisher Information，当有n个 i.i.d 观测值时，![I^*(\theta) = I(\theta)/n](https://www.zhihu.com/equation?tex=I%5E%2A%28%5Ctheta%29+%3D+I%28%5Ctheta%29%2Fn)。所以这时的直观解释就是，Fisher Information反映了我们对参数估计的准确度，它越大，对参数估计的准确度越高，即代表了越多的信息。







* [费雪信息 (Fisher information) 的直观意义是什么？](https://www.zhihu.com/question/26561604)